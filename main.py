# -*- coding: utf-8 -*-
"""AjakatlAI (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18DOhijCvyh7OQUaMKzpq4tMCe5zm9yaf
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers trl datasets openai unsloth langchain langchain-community langchain-chroma pypdf

from google.colab import drive

# Mount the Google Drive at /content/drive
drive.mount('/content/drive')

import os
from urllib.request import urlretrieve
import numpy as np
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import pipeline
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from unsloth import FastLanguageModel
from trl import SFTTrainer
from datasets import load_dataset
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
from openai import OpenAI
import torch
import json
import pandas as pd
from datasets import Dataset
from unsloth import PatchDPOTrainer
from transformers import TrainingArguments
from trl import DPOTrainer, DPOConfig
from openai import OpenAI
import json
from huggingface_hub import HfFolder
from datasets import config
max_seq_length = 4096*2 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    "unsloth/Llama-3.2-3B-Instruct",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = model.to('cuda')

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

ds = load_dataset("somosnlp-hackathon-2022/Axolotl-Spanish-Nahuatl", split="train")

promptspnh = """Traduce esto al Nahuatl: {} Nahuatl: {}"""
promptnhsp = """Traduce esto al Español: {} Español: {}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formating_input_nh(examples):
  spanish = examples["sp"]
  nahuatl = examples["nah"]
  texts = []
  for sp, nah in zip(spanish, nahuatl):
    text = promptspnh.format(sp, nah) + EOS_TOKEN
    texts.append((text))
  return {"text": texts}

datasetspnh = ds.map(formating_input_nh, batched=True)

def formating_input_sp(examples):
  spanish = examples["sp"]
  nahuatl = examples["nah"]
  texts = []
  for sp, nah in zip(spanish, nahuatl):
    text = promptnhsp.format(nah,sp) + EOS_TOKEN
    texts.append((text))
  return {"text": texts}

datasetnhsp = ds.map(formating_input_sp, batched=True)

datasetnhsp[0]

trainer1 = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = datasetspnh,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,

        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 50,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)

trainer2 = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = datasetnhsp,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,

        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 50,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)

for i in range(15):

  trainer1.train()
  trainer2.train()

FastLanguageModel.for_inference(model)
inputs = tokenizer("""Explica el siguiente documento en espanol: Kejatsa:
Tlanauatili tlen ika nopa tlajtlanijketl tlen kinextia para kichijtok nopa tlamantli tlen kiijtoua uan ya kichijtok nopa tlaxtlauili tlen kinamiki, kiselis nopa licencia de conducir tipo Motorist. Español:
""", return_tensors = "pt").to("cuda")
outputs = model.generate(**inputs)
tokenizer.batch_decode(outputs)

FastLanguageModel.for_inference(model)
inputs = tokenizer("Traduce al español: nokoltsin itoka Nick Español: ", return_tensors = "pt").to("cuda")
outputs = model.generate(**inputs)
tokenizer.batch_decode(outputs)

model.save_pretrained_merged("/content/drive/MyDrive/model", tokenizer, save_method = "merged_16bit",)

"""## **Langchain Workflow**"""



os.makedirs("docs", exist_ok=True)
files = [
    "https://www.aguascalientes.gob.mx/ssp/documentos/RequisitosLicencias.pdf",
    "https://portalanterior.ine.mx/archivos2/portal/credencial/pdf-credencial/pasos-en-el-modulo-INE2014.pdf",
]
for url in files:
    file_path = os.path.join("docs", url.rpartition("/")[2])
    urlretrieve(url, file_path)

loader = PyPDFDirectoryLoader("/content/docs")

docs_before_split = loader.load()
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 700,
    chunk_overlap  = 50,
)
docs_after_split = text_splitter.split_documents(docs_before_split)

docs_after_split[0]

avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)
avg_char_before_split = avg_doc_length(docs_before_split)
avg_char_after_split = avg_doc_length(docs_after_split)

print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')
print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')

huggingface_embeddings = HuggingFaceBgeEmbeddings(
    model_name="BAAI/bge-small-en-v1.5",  # alternatively use "sentence-transformers/all-MiniLM-l6-v2" for a light and faster experience.
    model_kwargs={'device':'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

sample_embedding = np.array(huggingface_embeddings.embed_query(docs_after_split[0].page_content))
print("Sample embedding of a document chunk: ", sample_embedding)
print("Size of the embedding: ", sample_embedding.shape)

vectorstore = Chroma.from_documents(docs_after_split, huggingface_embeddings)

query = """Que documentos llevo a mi tramite de credencial INE"""
# Sample question, change to other questions you are interested in.
relevant_documents = vectorstore.similarity_search(query)
print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query. Display the first one:\n')
print(relevant_documents[0].page_content)

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})



base_url = "https://api.aimlapi.com/v1"
api_key = "d2ac18e7419349149ca9a84f39449e74"

api = OpenAI(api_key=api_key, base_url=base_url)

query = "Que documentos necesito para tramitar mi INE"
docs = vectorstore.similarity_search(query)
context = ""
for document in docs:
  context += document.page_content

system_prompt = "Responde las preguntas en base al contexto dado"

user_prompt = "Que documentos necesito para tramitar mi INE" + context

completion = api.chat.completions.create(
      model="meta-llama/Llama-3.2-3B-Instruct-Turbo",
      messages=[
          {"role": "system", "content": system_prompt},
          {"role": "user", "content": user_prompt},
      ],
      temperature=0.7,
      max_tokens=2048,
  )

system_prompt = """Generame 25 preguntas en formato JSON sobre el texto compartido. Que se vea asi:
                    JSON:
                    {
                      {
                        "question": "Pregunta 1",
                        "answer": "Respuesta 1"
                      },
                      {
                        "question": "Pregunta 2",
                        "answer": "Respuesta 2"
                      },
                      ...
                      }
                    }
                    """


user_prompt = """Conoce el trámite en el Módulo
para obtener tu Credencial para Votar
Si tienes una Cita te indicarán el escritorio
de atención que te corresponde, si no la
programaste, te asignarán un turno.
1 Al entrar al Módulo te solicitarán tus tres
documentos:
y te preguntarán qué trámite realizarás:
• Inscripción, cambio de domicilio, corrección de
datos personales o en dirección; Reposición o
Reimpresión; Reincorporación, Reemplazo por
pérdida de vigencia.
1) Documento que acredite
tu nacionalidad
2) Comprobante de domicilio
3) Identificación con foto
Sin ellos no podrás realizar ningún trámite.
Deberán ser originales, sin tachaduras ni
enmendaduras.
Revisa los medios de identificiación autorizados
en la página www.ine.mx
Derechos Reservados © Instituto Nacional Electoral, 2014.
Conoce el trámite en el Módulo
2 Capturarán tus datos en el sistema:
• Nombre y domicilio actual
• Todas tus huellas dactilares
• Fotografía digital
• Firma digital
• Se conservará copia
digital de tus
documentos
Capturarán las huellas
de todos tus dedos
Deberás
firmar
en la tableta
digital
Te tomarán una
fotografía digital Digitalizarán
tus documentos
Te mostrarán el formato impreso para revisar
tus datos, es importante que estén correctos
pues así quedarán registrados en tu nueva
credencial.
Deberás poner tu firma y huella autógrafa
(de puño y letra), en los recuadros asignados.
Derechos Reservados © Instituto Nacional Electoral, 2014.
Conoce el trámite en el Módulo
3
4
Finalmente, te entregarán un comprobante con
la fecha a partir de la cual podrás recoger tu
credencial.
Consérvalo, ya que deberás
presentarlo en el módulo
cuando vayas por ella.
A partir de la fecha que indica tu comprobante,
acude al mismo módulo donde realizaste tu
trámite para recogerla.
Identifícate con tus huellas
dactilares y firma de recibido.

I. Introducción
Estimado contribuyente, el Servicio de Administración Tributaria pone a tu
disposición el servicio de Inscripción al RFC con CURP para que realices a través del
Portal del SAT, tu inscripción en el Registro Federal de Contribuyentes (RFC) para lo
cual, es importante que tengas en cuenta lo siguiente:
• Este servicio inicia y concluye a través del Portal del SAT, sin necesidad de
acudir a las oficinas del SAT ni presentar ningún tipo de documento.
• Los requisitos para obtener tu RFC a través de esta opción son:
✓ Tener CURP vigente.
✓ Ser mayor de edad.
✓ Capturar en el formulario de inscripción, los datos de tu domicilio,
correo electrónico y características fiscales.
• Esta opción de inscripción podrá realizarse una sola vez.
• La clave del RFC que se genere en esta inscripción será válida para cualquier
trámite ante el SAT, otras autoridades de gobierno, así como para gestiones
administrativas que realices.
II. Inscripción en el RFC con CURP
En este apartado se orienta a las personas físicas, que desean efectuar su inscripción
en el RFC, a través de la CURP
Paso 1.- Ingresa a la liga siguiente:
https://www.sat.gob.mx/tramites/operacion/28753/obten-tu-rfc-con-la-clave-unicade-registro-de-poblacion-curp
O bien a https://www.sat.gob.mx/personas apartado Trámites del RFCPaso 2.- Del apartado Inscripción al RFC selecciona Obtén tu RFC con la Clave
Única de Registro de Población CURP; coloca el cursor sobre el título y da clic.
Antes de que comiences con la captura de la información, es importante que tomes
en cuenta que:
a) Al inicio de cada pantalla aparecerán instrucciones, que te servirán como
apoyo para la captura de tu información. Léelas cuidadosamente.
b) Existen campos marcados con [*] asterisco, que son obligatorios, por lo que no
pueden quedar vacíos.
c) Algunos de los campos tienen el símbolo [?], en ellos encontrarás ayuda para
la captura.
d) Desliza las barras lateral y horizontal para visualizar todos los campos del
formulario.
Paso 3.- Captura tus datos de identificación, CURP, e introduce el código captcha
(texto de la imagen) respetando mayúsculas y minúsculas, si ésta no es clara,
presiona en el icono para cambiarla y captura nuevamente, da clic en Continuar.
Si requieres verificar los datos de tu CURP puedes ingresar a la
página http://consultas.curp.gob.mx para obtener mayor información.
Paso 4.- Una vez capturada la CURP y los caracteres de la imagen, presiona el
botón Continuar.
De la validación de tu CURP pueden surgir los siguientes mensajes:
a) Si tu CURP ya está asociada a un RFC:
Nota: Para obtener mayor información relacionada con el registro, ingresa a la
siguiente liga : https://www.sat.gob.mx/aplicacion/operacion/31274/consultatu-clave-de-rfc-mediante-curp o puedes acudir a la Administración
Desconcentrada de Servicios al Contribuyente, de tu preferencia, con una
identificación oficial, previa cita.
b) Si tu CURP no es correcta o no está completa:
Nota: Captura nuevamente las letras y números de tu CURP.
c) Si la CURP corresponde a un menor de edad:
Nota: Para la inscripción de un menor de edad, deberá presentarse en la
Administración Desconcentrada de Servicios al Contribuyente de tu preferencia, o
bien, si se encuentra dentro del supuesto siguiente Las personas físicas menores
de edad a partir de los 16 años pueden inscribirse en el RFC, siempre que presten
exclusivamente un servicio personal subordinado (salarios) desde su inscripción
y hasta que tengan 18 años cumplidos, sin que puedan cambiar de régimen
fiscal hasta que cumplan la mayoría de edad; deberán ingresar a la liga siguiente:
https://www.sat.gob.mx/tramites/83843/rfc-con-la-clave-unica-de-registro-depoblacion-(curp),-para-personas-fisicas-menores-de-edad-a-partir-de-16-anos"""

api = OpenAI(api_key=api_key, base_url=base_url)

completion = api.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
        max_tokens=2048,
    )


preguntas = []

for i in range(2):

  completion = api.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
        max_tokens=2048,
    )

  json_string = completion.choices[0].message.content.split("```json")[1].split("```")[0]
  data = json.loads(json_string)

  preguntas += [item['question'] for item in data['questions']]

pairs = []
for pregunta in preguntas:

  docs = vectorstore.similarity_search(pregunta)
  context = ""
  for document in docs:
    context += document.page_content

  system_prompt = "Responde las preguntas en base al contexto dado"

  user_prompt = "Que documentos necesito para tramitar mi INE" + context

  for _ in range(5):

    completion = api.chat.completions.create(
        model="meta-llama/Llama-3.2-3B-Instruct-Turbo",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
        max_tokens=2048,
    )

    pairs.append((pregunta,completion.choices[0].message.content ))

columns = ["Pregunta", "Respuesta"]

df = pd.DataFrame(pairs, columns = columns)

def generar_pares_respuestas(json_string):
    # Convertir el string JSON a diccionario
    try:
        # Reemplazar corchetes externos por llaves y agregar comillas a "respuestas"
        json_string_corregido = json_string.replace('[{', '{"respuestas": [{').replace('}]', '}]}')
        json_data = json.loads(json_string_corregido)
    except json.JSONDecodeError as e:
        print(f"Error al decodificar JSON: {e}")
        return []

    # Extraer la lista de respuestas del JSON
    respuestas = json_data['respuestas']

    # Crear pares de respuestas donde la primera tenga mejor calificación
    pares_respuestas = []

    # Generar todas las combinaciones posibles de respuestas
    for (j, response_j), (k, response_k) in itertools.product(
        enumerate(respuestas),
        enumerate(respuestas)
    ):
        # Asegurar que j y k sean diferentes
        if j != k:
            # Verificar que la calificación de j sea mayor que la de k
            if response_j['calificacion'] > response_k['calificacion']:
                par = {
                    'response_j': response_j['respuesta'],
                    'response_k': response_k['respuesta']
                }
                pares_respuestas.append(par)

    return pares_respuestas

dpo_data = []

for j, pregunta in enumerate(df['Pregunta'].unique()):
    respuestas = df.loc[df['Pregunta'] == pregunta, 'Respuesta']

    comptext = "Compara las siguientes respuestas para la siguiente pregunta " + pregunta

    comptext += """y calificalas en un JSON de la siguiente forma. Solo regresa el JSON:
                {
                  "respuestas": [
                    {
                      respuesta: "Transcribe la respuesta... ",
                      calificacion: (Entero del 1 al 10)
                    }
                  ]
                }

                Respuestas:
                """

    for i, respuesta in enumerate(respuestas):

      comptext += f"Respuesta {i}: " + respuesta + "Fin de la respuesta"


    completion = api.chat.completions.create(
        model="anthropic/claude-3.5-sonnet-20241022",
        messages=[
            {"role": "user", "content": comptext}
        ],
        temperature=0.7,
        max_tokens=2048,
    )

    json_data = completion.choices[0].message.content

    pairs = generar_pares_respuestas(json_data)

    for sample in pairs:

      s = [pregunta] + list(sample.values())
      dpo_data.append(s)

    print(f"Pregunta {j} lista")

columns = ["question", "response_j", "response_k"]

dpodf = pd.DataFrame(dpo_data, columns=columns)

dpodf.to_csv("DPOData.csv")

dpodf = pd.read_csv("DPOData.csv")

dpodf

"""# **DPO Training**"""

dataset = Dataset.from_pandas(dpodf)


PatchDPOTrainer()

def return_prompt_and_responses(samples):
    return {
        "prompt": [
            "Question: " + question + "\n\nAnswer: "
            for question in samples["question"]
        ],
        "chosen": samples["response_j"],   # rated better than k
        "rejected": samples["response_k"], # rated worse than j
    }

original_columns = dataset.column_names

dataset = dataset.map(
    return_prompt_and_responses,
    batched=True,
    remove_columns=original_columns
)

dataset[0]

training_args = DPOConfig(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 100,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
)

dpo_trainer = DPOTrainer(
    model,                 # base model from SFT pipeline
    None,             # typically a copy of the SFT trained base model
    beta=0.1,              # temperature hyperparameter of DPO
    train_dataset=dataset, # dataset prepared above
    tokenizer=tokenizer,   # tokenizer
    args=training_args,    # training arguments e.g. batch size, lr, etc.
)

#Final Learning Step
for i in range(5):
  dpo_trainer.train()